{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","from torch import optim, nn, utils, Tensor\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor\n","import lightning as L\n","\n","# define any number of nn.Modules (or use your current ones)\n","encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n","decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n","\n","\n","# define the LightningModule\n","class LitAutoEncoder(L.LightningModule):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def training_step(self, batch, batch_idx):\n","        # training_step defines the train loop.\n","        # it is independent of forward\n","        x, _ = batch\n","        x = x.view(x.size(0), -1)\n","        z = self.encoder(x)\n","        x_hat = self.decoder(z)\n","        loss = nn.functional.mse_loss(x_hat, x)\n","        # Logging to TensorBoard (if installed) by default\n","        self.log(\"train_loss\", loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n","        return optimizer\n","\n","\n","# init the autoencoder\n","autoencoder = LitAutoEncoder(encoder, decoder)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 9.91M/9.91M [00:11<00:00, 836kB/s] \n","100%|██████████| 28.9k/28.9k [00:00<00:00, 54.0kB/s]\n","100%|██████████| 1.65M/1.65M [00:28<00:00, 57.8kB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 1.55MB/s]\n"]}],"source":["# defining dataset\n","dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n","train_loader = utils.data.DataLoader(dataset)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","GPU available: True (mps), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","\n","  | Name    | Type       | Params | Mode \n","-----------------------------------------------\n","0 | encoder | Sequential | 50.4 K | eval \n","1 | decoder | Sequential | 51.2 K | train\n","-----------------------------------------------\n","101 K     Trainable params\n","0         Non-trainable params\n","101 K     Total params\n","0.407     Total estimated model params size (MB)\n","4         Modules in train mode\n","4         Modules in eval mode\n","/Users/aryamantepal/anaconda3/envs/mnist-env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9: 100%|██████████| 100/100 [00:01<00:00, 99.52it/s, v_num=1] "]},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=10` reached.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9: 100%|██████████| 100/100 [00:01<00:00, 98.57it/s, v_num=1]\n"]}],"source":["# train the model \n","trainer = L.Trainer(limit_train_batches=100, max_epochs=10)\n","trainer.fit(model=autoencoder, train_dataloaders=train_loader)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \n","Predictions (4 image embeddings):\n"," tensor([[ 0.1692, -0.0091, -0.4121],\n","        [ 0.2443,  0.0366, -0.3784],\n","        [ 0.2575,  0.0385, -0.4281],\n","        [ 0.2535,  0.0647, -0.4903]], device='mps:0',\n","       grad_fn=<LinearBackward0>) \n"," ⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡\n"]}],"source":["import torch\n","\n","# load checkpoint\n","checkpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt\"\n","autoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n","\n","# choose your trained nn.Module\n","encoder = autoencoder.encoder\n","encoder.eval()\n","\n","# embed 4 fake images!\n","fake_image_batch = torch.rand(4, 28 * 28, device=autoencoder.device)\n","embeddings = encoder(fake_image_batch)\n","print(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"ML-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
